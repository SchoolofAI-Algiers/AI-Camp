{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convert text to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove numbers**\n",
    "\n",
    "Remove numbers if they are not relevant to your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "def remove_numbers(my_str):\n",
    "    # Numbers\n",
    "    numbers = \"0123456789\" \n",
    "    \n",
    "    # New text\n",
    "    new_str = \"\" \n",
    "    \n",
    "    # Loop on every character in my_str \n",
    "    for char in my_str:\n",
    "        # Exclude the numbers\n",
    "        if char not in numbers:\n",
    "            # Create the new string\n",
    "            new_str = new_str + char\n",
    "    return new_str\n",
    "\n",
    "# Define your text\n",
    "before_text = 'Hello123 World456!'\n",
    "\n",
    "# Clean it calling remove_numbers function\n",
    "after_text = remove_numbers(before_text)\n",
    "\n",
    "# Print the result\n",
    "print(after_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuations(my_str):\n",
    "    # Punctuations\n",
    "    punctuations = \"'!()-[};:'/^@#_][%.?\" \n",
    "    \n",
    "    # New text\n",
    "    new_str = \"\" \n",
    "    \n",
    "    # Loop on every character in my_str \n",
    "    for char in my_str:\n",
    "        # Exclude the punctuations\n",
    "        if char not in punctuations:\n",
    "            # Create the new string\n",
    "            new_str = new_str + char\n",
    "    return new_str\n",
    "\n",
    "\n",
    "# Define your text\n",
    "before_text = 'This is an example of string with punctuation!'\n",
    "\n",
    "# Clean it calling remove_punctuations function\n",
    "after_text = remove_punctuations(before_text)\n",
    "\n",
    "# Print the result\n",
    "print(after_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "# SPACY:\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc=\"I'd love to traval to LA 7th ocotber with 100$ with Mr.Jhon\"\n",
    "doc = nlp(stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I | 'd | love | to | traval | to | LA | 7th | ocotber | with | 100 | $ | with | Mr. | Jhon | "
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'd\", 'love', 'to', 'traval', 'to', 'paris', 'the', '7th', 'ocotber', 'with', '100$']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'd love to traval to paris the 7th ocotber with 100$\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'd love to traval to LA 7th ocotber with 100$ with Mr. Jhon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'d\", 'love', 'to', 'traval', 'to', 'LA', '7th', 'ocotber', 'with', '100', '$', 'with', 'Mr.', 'Jhon']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'd\", 'love', 'to', 'traval', 'to', 'LA', '7th', 'ocotber', 'with', '100$', 'with', 'Mr.', 'Jhon']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy STOP WORDS :\n",
    "\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.\n",
    "\n",
    "this boy is good ==> boy good. ( We kept the meaning and we reduced the size of the sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'i went to this restaurent and i felt that the food was horrible'\n",
    "text2 = \"it's tasty, the service is amazing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us', 'rather', 'get', 'his', 'at', 'when', 'whereupon', 'had', 'doing', 'whither', '’d', 'even']\n",
      "['became', 'latterly', 'an', 'more', 'using', 'within', 'should', 'something', 'everyone', 'around', 'ten', 'sometimes']\n",
      "['almost', 'wherever', 'empty', 'did', 'towards', 'so', 'meanwhile', 'various', 'been', 'anyway', 'nine', 'yet']\n",
      "['afterwards', 'off', \"'ve\", 'throughout', 'how', 'through', 'up', 'front', 'really', 'any', 'your', 'this']\n",
      "['beforehand', '’s', 'does', 'which', 'he', 'although', 'be', 'between', 'regarding', 'everything', 'elsewhere', 'since']\n",
      "['seem', 'a', \"n't\", 'last', 'somehow', 'our', 'yours', 'my', 'thereafter', 'else', 'ourselves', 'just']\n",
      "['itself', 'put', 'on', 'himself', 'together', 'whence', 'against', 'often', 'during', 'him', 'otherwise', 'sometime']\n",
      "['seeming', '’ve', 'am', 'serious', 'onto', '‘ll', 'anyone', 'namely', 'its', 'what', '‘d', 'besides']\n",
      "['hence', 'yourselves', 'ever', 'anywhere', 'toward', 'whether', 'several', 'fifty', 'becoming', 'perhaps', 'third', 'nevertheless']\n",
      "['full', 'she', 'hereafter', 'go', 'anything', 'might', 'hereupon', '’ll', 'please', 'back', 'still', 'n’t']\n",
      "['show', 'over', 'fifteen', 'quite', 'because', 'other', 'whatever', 'about', 'another', 'seems', 'herself', 'side']\n",
      "['thru', 'twelve', 'few', 'always', 'only', 'thus', 'three', 'will', 'via', '’m', 'was', 'nobody']\n",
      "['wherein', 'without', 'none', 'per', 'along', 'call', 'also', 'make', 'where', 'name', 'ca', 'myself']\n",
      "['next', 'moreover', 'latter', 'being', 'move', 'due', 'every', 'by', 'both', 'some', 'others', 'alone']\n",
      "['while', \"'re\", 'less', 'can', 'then', 'somewhere', 'i', '‘m', 'amount', 'nor', 'from', 'either']\n",
      "['seemed', 'all', 'made', 'with', 'in', 'forty', 'these', 'enough', 'much', 'therefore', 'never', 'own']\n",
      "['for', 'too', 'under', 'above', 'nothing', 'whereas', 'of', 'thereupon', 'used', 'into', 'five', 'whoever']\n",
      "['already', 'each', '‘re', 'amongst', 'their', 'are', 'who', 'whom', 'whereafter', 'one', 'indeed', 'bottom']\n",
      "['formerly', 'keep', 'same', 'however', 'least', \"'d\", 'many', 'after', 'eleven', '’re', 'no', 'give']\n",
      "['do', 'must', 'everywhere', 'hundred', 'and', 'were', 'former', 'they', 'down', 'until', 'someone', 'part']\n",
      "['again', 'yourself', 'whole', 'out', 'we', 'six', 'across', 'it', 'herein', 'among', 'unless', 'anyhow']\n",
      "['thereby', 'mine', 'thence', '‘ve', 'sixty', 'take', 'twenty', 'top', 'whereby', 'her', 'ours', 'first']\n",
      "['once', 'very', 'you', 'becomes', 'eight', 'would', 'themselves', 'except', 'may', 'or', 'me', 'such']\n",
      "['neither', 'done', 'though', 'become', 'hereby', 'there', '‘s', 'has', 'further', 'well', 'two', 'below']\n",
      "['most', 'whose', \"'ll\", 'here', 'say', 'is', 'could', 'four', 'but', 'not', 'the', 'than']\n",
      "['mostly', 'upon', 'cannot', 'have', 'nowhere', 'beyond', 'as', 'n‘t', \"'m\", 'hers', 're', 'if']\n",
      "['whenever', \"'s\", 'them', 'therein', 'to', 'noone', 'beside', 'before', 'those', 'behind', 'now', 'why']\n",
      "['that', 'see']\n"
     ]
    }
   ],
   "source": [
    "# print(nlp.Defaults.stop_words)\n",
    "mylist = list(nlp.Defaults.stop_words)\n",
    "for i in range(0, len(mylist), 12):\n",
    "    print(mylist[i:i + 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['party'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['party'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['party'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'its', 'mustn', 'his', 'at', 'what', 'when', 'had', 'doing', 'with', 's', 'in', 'yourselves', 'her', 'ours', 'shan', 'once', 'an', 'very', 'these', 'more', 'you', 'don', 'should', \"she's\", 'own', \"shouldn't\", 'for', 'themselves', 'mightn', 'she', \"you've\", 'too', 'did', 'under', \"won't\", 'doesn', 'or', 'above', 'so', \"shan't\", 'y', 'me', 'such', 'been', \"should've\", \"don't\", 'of', \"mightn't\", \"you'd\", 'won', 'off', 'into', 'there', 'each', 'shouldn', 'how', 'through', 't', 'up', 'over', 'having', 'their', 'are', 'because', 'm', 'who', 'has', 'any', 'other', 'couldn', 'further', 'didn', 'whom', 'your', 'this', 'about', 'ain', 'below', \"wasn't\", 'herself', 'which', 'does', 'weren', 'same', 'he', \"you're\", 'be', 'between', 'most', 'wasn', 'isn', 'few', 'only', 'will', 'was', 've', 'after', \"wouldn't\", 'here', 'hadn', 'a', 'aren', 'is', \"that'll\", 'no', \"hasn't\", \"isn't\", \"needn't\", 'do', 'but', 'where', 'ma', \"aren't\", 'the', 'were', 'and', 'myself', 'not', 'they', 'our', 'than', 'yours', 'my', 'down', \"hadn't\", 'until', 'have', \"didn't\", 'd', 'as', 'ourselves', 'just', 'being', 'hers', 'needn', 'itself', \"couldn't\", 're', 'if', \"mustn't\", 'on', 'himself', 'by', 'again', 'them', 'both', 'some', \"doesn't\", 'haven', 'yourself', 'out', 'against', 'we', 'theirs', 'during', 'it', 'him', 'o', 'to', \"you'll\", \"haven't\", 'wouldn', 'while', \"it's\", 'can', 'then', 'i', 'before', 'those', 'hasn', 'am', 'nor', 'now', 'll', 'why', \"weren't\", 'that', 'from'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a sample sentence, and it shows off the stop words filtration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'and', 'it', 'shows', 'off', 'the', 'stop', 'words', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'sentence', ',', 'shows', 'stop', 'words', 'filtration']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in word_tokens if w.lower() not in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = set(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ما انفك | أخبر | هاته | حاء | مايو | خال | مارس | أولاء | ّأيّان | ريال | ذلكم | نحو | هيهات | لنا | ا | حيَّ | ذال | زعم | صباح | ليت | حَذارِ | بي | خلا | راء | تسعة | جميع | بنا | ذين | أنى | فإذا | بماذا | منها | بؤسا | شرع | بهم | ذَيْنِ | ق | زاي | ذلكن | واحد | ستين | غداة | والذي | اللتيا | تلك | رويدك | صار | هلا | وا | تلقاء | عين | أمام | انقلب | معاذ | أرى | عشرة | علق | غدا | أولالك | ؤ | هن | بطآن | إلّا | تجاه | كلَّا | بَسْ | اللذان | هيا | إنا | ذلكما | فيه | تلكما | كيفما | مازال | كأنما | لهن | ليسوا | تعلَّم | راح | رزق | كلّما | هاء | طَق | شبه | يفعلان | آض | ته | رابع | فوق | حمدا | بها | جويلية | لسنا | مكانَك | كلتا | أعطى | سنتيم | ثم | أما | أف | آهِ | أو | غالبا | اللتان | ذوا | م | حدَث | عشرين | وإذ | دينار | عل | س | آهٍ | يوليو | ولو | سحقا | صاد | صراحة | سوى | يا | هؤلاء | حيث | ض | فإن | هي | نيف | ثمانين | تي | الآن | دون | ذا | ليس | هَؤلاء | بيد | تلكم | كأي | أنّى | خمسين | لكم | ثلاث | جعل | لن | تبدّل | ثلاثمائة | ما | أغسطس | هما | عوض | ذيت | بسّ | مهما | ثلاثاء | أمّا | ذانك | أربعاء | سقى | هكذا | ثالث | إلى | يفعلون | علًّ | كرب | لام | أم | حبذا | أحد | مكانكما | أكتوبر | أولئك | ثامن | أى | هاهنا | هذا | ألا | ك | لكنَّ | لست | أوشك | فيها | فرادى | بين | هيّا | أكثر | أبريل | فو | مافتئ | أُفٍّ | لكما | الذي | ليست | أربعة | هللة | صهٍ | تخذ | سبعون | استحال | باء | مثل | خاصة | عند | إليكما | أوّهْ | خمسمائة | أي | خلف | تسعمئة | نبَّا | ذان | ولا | كانون | ياء | طاق | الذين | يناير | خبَّر | آ | سبتمبر | مكانكنّ | أيا | تارة | كاد | شتانَ | بات | ثاني | سادس | خميس | إياكم | تانِ | أخذ | آي | هبّ | كسا | دولار | أيضا | كان | هنالك | تسعون | بعد | تين | اللاتي | الألاء | تموز | ذينك | فاء | كم | أين | هذي | ف | ذات | لي | حتى | إحدى | د | حسب | ذواتا | أنتِ | صدقا | مذ | شَتَّانَ | أنبأ | فيما | تاسع | ص | هاك | كيت | أيار | هَذانِ | سابع | أنت | عَدَسْ | هم | تينك | ذهب | بكم | هل | ابتدأ | على | إياكما | فلا | عامة | رجع | عسى | لستما | أوه | إيهٍ | سبعة | وما | تانِك | هيت | لم | إليك | أفٍّ | وراءَك | اتخذ | و | بل | لهما | غين | عدَّ | أول | فبراير | إمّا | فلس | عما | تَيْنِ | ثان | نحن | كأيّ | ليسا | فلان | فيم | بهن | لئن | به | كلا | لا | ذواتي | أنا | حادي | بكن | نوفمبر | تسع | لوما | إليكنّ | إلا | أبو | ذانِ | لدن | من | ذِي | لستم | تحت | في | ريث | هو | ظاء | اللائي | طرا | هَذَيْنِ | مليم | ثمّ | هلّا | لكيلا | حار | هناك | إى | عشرون | ما أفعله | لما | ذ | خمسمئة | أهلا | عيانا | ظلّ | أيّان | كأنّ | ح | أفريل | رأى | إياك | هَاتِه | ظ | لها | عدا | وُشْكَانَ | خمسة | ع | تِي | ل | إيه | سبع | كيف | فضلا | خامس | شباط | آنفا | هاتين | أمامكَ | اللذين | ها | شتان | هاكَ | وإن | صبرا | ثمان | أنًّ | جيم | أنتما | إنه | هَاتانِ | ساء | كأين | ثمنمئة | آه | واو | أضحى | رُبَّ | ستون | درهم | وَيْ | حاشا | عليه | منه | أقبل | ألف | لكن | كأيّن | هَذا | إذاً | لك | كلما | شيكل | إي | سوف | أوت | بس | أيّ | عن | كذلك | غير | تفعلون | قرش | لولا | سرا | ثلاثة | واهاً | ذاك | اللواتي | وإذا | كاف | هَذِي | قلما | ليرة | أمس | ش | ليستا | مه | أجمع | لهم | دواليك | قطّ | ب | جوان | صبر | اللتين | كأن | كلاهما | عاد | ماي | أيلول | ستة | هَاتِي | ما برح | أصلا | فيفري | بك | ست | نَخْ | جلل | نا | بئس | جانفي | حرى | آها | بضع | ذو | ممن | بما | أل | ماذا | سمعا | مساء | إياهم | ذي | مئة | ومن | ذلك | علم | كليهما | لا سيما | ضحوة | إياها | كل | إذما | أربع | آمينَ | تسعين | ثمانية | كن | اربعون | خلافا | نفس | مرّة | قام | والذين | اثنا | بخ | ين | سرعان | أجل | طاء | ميم | ط | كذا | لستن | إذن | غ | ت | إليكن | ضاد | تِه | غادر | هَذِه | تاء | يونيو | صهْ | ثمَّ | حين | بكما | لعمر | مائة | إنما | ء | نون | كما | عجبا | تفعلان | أصبح | أمسى | ثمّة | يورو | ولكن | خاء | مع | متى | طفق | مما | شين | كثيرا | عليك | كي | أخو | حيثما | اثنين | إذا | أسكن | ذِه | له | دال | أقل | إياه | ءَ | لدى | كِخ | لكي | بعض | خ | أعلم | إزاء | خمس | جمعة | إن | ه | اثنان | أفعل به | هنا | مادام | بهما | أمد | شمال | هذان | أينما | ن | لسن | التي | حبيب | أمامك | لو | فمن | إذ | ز | سبحان | أبٌ | قبل | بلى | ى | ئ | عشر | سبعمائة | همزة | تسعمائة | نيسان | عاشر | ثاء | ارتدّ | ج | بمن | أربعمائة | خمسون | كى | دونك | إياهن | إياي | هاتان | ثمانون | ظنَّ | إليكم | أنتن | حزيران | يمين | ذه | إما | هَجْ | سبعمئة | تشرين | ثمانمئة | منذ | إلَيْكَ | ر | حمو | مئتان | سبت | ستمئة | لعلَّ | حقا | اثني | لاسيما | مكانكم | إيانا | جير | بغتة | لبيك | هلم | ثلاثمئة | ث | أ | ستمائة | كليكما | أربعمئة | آناء | إياهما | آهاً | ة | لات | بخٍ | ثلاثين | هاتي | أطعم | قد | سين | تعسا | قاف | بَلْهَ | يوان | الألى | أيها | ثماني | وهب | أن | إنَّ | لكنما | ثمة | زود | لمّا | قاطبة | تحوّل | اخلولق | جنيه | أنشأ | ترك | بعدا | نَّ | وهو | درى | ثلاثون | لعل | أخٌ | تفعلين | أنتم | ألفى | انبرى | هذه | آب | إياكن | هَيْهات | ديسمبر | اربعين | حجا | آذار | طالما | حمٌ | سبعين | حاي | إليكَ | نعم | ورد | وجد | أبدا | ي | هَاتَيْنِ | هذين'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ar))\n",
    "' | '.join(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arabicstopwords.arabicstopwords as stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stp.is_stop('ممكن')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stp.is_stop(u'منكم')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generous --> gener\n",
      "---------------------------------------\n",
      "generation --> generat\n",
      "generation --> gener\n",
      "---------------------------------------\n",
      "generously --> generous\n",
      "generously --> gener\n",
      "---------------------------------------\n",
      "generate --> generat\n",
      "generate --> gener\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  is    has setmming      is\n",
      "Word  was    has setmming      wa\n",
      "Word  be    has setmming      be\n",
      "Word  been    has setmming      been\n",
      "Word  are    has setmming      are\n",
      "Word  were    has setmming      were\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  is    has setmming      is\n",
      "Word  was    has setmming      was\n",
      "Word  be    has setmming      be\n",
      "Word  been    has setmming      been\n",
      "Word  are    has setmming      ar\n",
      "Word  were    has setmming      wer\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"book\",\"booking\",\"booked\",\"books\",\"booker\",\"bookstore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  book    has setmming      book\n",
      "Word  booking    has setmming      book\n",
      "Word  booked    has setmming      book\n",
      "Word  books    has setmming      book\n",
      "Word  booker    has setmming      booker\n",
      "Word  bookstore    has setmming      bookstor\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  book    has setmming      book\n",
      "Word  booking    has setmming      book\n",
      "Word  booked    has setmming      book\n",
      "Word  books    has setmming      book\n",
      "Word  booker    has setmming      book\n",
      "Word  bookstore    has setmming      bookst\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'had you booked the air booking yet ? if not try to book it ASAP since booking will be out of books'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  had    has setmming      had\n",
      "Word  you    has setmming      you\n",
      "Word  booked    has setmming      book\n",
      "Word  the    has setmming      the\n",
      "Word  air    has setmming      air\n",
      "Word  booking    has setmming      book\n",
      "Word  yet    has setmming      yet\n",
      "Word  ?    has setmming      ?\n",
      "Word  if    has setmming      if\n",
      "Word  not    has setmming      not\n",
      "Word  try    has setmming      tri\n",
      "Word  to    has setmming      to\n",
      "Word  book    has setmming      book\n",
      "Word  it    has setmming      it\n",
      "Word  ASAP    has setmming      asap\n",
      "Word  since    has setmming      sinc\n",
      "Word  booking    has setmming      book\n",
      "Word  will    has setmming      will\n",
      "Word  be    has setmming      be\n",
      "Word  out    has setmming      out\n",
      "Word  of    has setmming      of\n",
      "Word  books    has setmming      book\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\n",
    "             \"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),ls.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "'d \t AUX \t 6992604926141104606 \t would\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "yesterday \t NOUN \t 1756787072497230782 \t yesterday\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "doc1 = nlp(u\"I am a runner running in a race because I'd love to run since I ran yesterday\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t','\\t', token.lemma_)\n",
    "    # POS == Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp(u'cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nlp(u'pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6205331191153399"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'lion cat pet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion   lion 1.0\n",
      "lion   cat 0.3854507803916931\n",
      "lion   pet 0.20031583309173584\n",
      "cat   lion 0.3854507803916931\n",
      "cat   cat 1.0\n",
      "cat   pet 0.732966423034668\n",
      "pet   lion 0.20031583309173584\n",
      "pet   cat 0.732966423034668\n",
      "pet   pet 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1, ' ', token2, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9458659139284967"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('i love school').similarity(nlp('i hate school'))\n",
    "# it take the mean between the 3 words \n",
    "# so we dont use WE in this case instead we use sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5212638134444677"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('love').similarity(nlp('like'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0565e+00, -3.2259e+00, -5.7364e+00, -6.1460e+00,  1.5748e-01,\n",
       "       -2.4284e+00,  7.6580e+00,  2.7064e+00, -2.2110e+00, -8.9990e-01,\n",
       "        6.7584e+00, -2.6983e+00, -7.6898e+00,  2.4036e+00,  7.9365e+00,\n",
       "       -2.1374e+00, -1.7134e-01, -1.9848e+00,  2.1000e+00,  2.0230e+00,\n",
       "       -1.1329e-01,  3.7908e+00, -3.3405e+00, -8.5698e+00,  3.6204e+00,\n",
       "        9.6741e-01, -2.4264e+00,  4.4687e+00,  1.5334e+00,  1.3886e+00,\n",
       "        1.4789e+00, -4.5457e+00, -1.0838e+00, -1.9183e+00,  3.3245e+00,\n",
       "       -1.4215e-01, -1.9783e+00,  5.4134e-01,  1.9844e+00, -7.2322e-02,\n",
       "       -1.3614e+00,  3.2423e+00,  3.5776e+00, -2.6719e+00,  1.0900e+00,\n",
       "        4.6470e+00, -2.1616e+00, -3.8358e+00,  4.1603e+00,  4.6552e+00,\n",
       "       -1.6725e+00, -4.4985e+00, -2.8982e-01, -4.5826e+00, -4.6451e-02,\n",
       "       -4.6299e-01,  8.8783e-01,  1.2574e+00,  6.6601e+00,  5.5503e+00,\n",
       "        3.5401e+00,  2.6721e-01,  3.7113e+00, -4.0592e+00,  9.4553e-01,\n",
       "        5.9527e+00, -4.1922e+00, -6.9438e+00,  5.0857e+00,  1.4050e+00,\n",
       "       -2.4574e+00,  7.3218e+00, -2.1215e+00, -6.7814e-01, -2.2871e+00,\n",
       "        1.8938e+00, -2.5832e+00,  4.3815e+00,  4.2151e+00, -3.6183e+00,\n",
       "       -1.4703e+00, -1.3201e+00,  2.6590e+00, -5.6996e+00, -2.0867e-02,\n",
       "        9.6205e-01,  2.0081e+00,  1.4750e+00, -7.7691e+00, -3.4898e+00,\n",
       "        3.8443e+00,  3.2584e+00, -6.3080e-01, -8.3240e+00, -3.6071e+00,\n",
       "       -5.0766e+00,  1.5934e+00, -2.2644e+00, -1.5554e+00, -2.9191e+00,\n",
       "       -1.8064e-01,  6.5120e-01, -2.9984e-01,  4.0194e+00,  2.2026e-01,\n",
       "        8.0274e-01, -1.7585e+00,  1.8378e+00, -3.2169e+00,  2.6540e+00,\n",
       "        2.3664e+00, -6.5790e+00,  6.5348e-01, -1.3508e+00,  2.1475e+00,\n",
       "        3.2138e+00, -8.9947e+00, -7.6404e+00, -1.2826e+00, -1.0916e+00,\n",
       "       -4.8551e+00,  2.3193e+00, -5.1803e+00,  7.3055e-01, -1.5054e+00,\n",
       "        3.5798e-01,  4.8880e+00, -3.3836e+00,  8.9427e-01, -2.6204e+00,\n",
       "       -3.7773e+00,  2.2107e+00, -4.8519e+00, -9.8902e-01, -3.3669e+00,\n",
       "        2.7894e+00, -5.0841e+00, -5.7924e-01,  3.9221e+00, -2.8150e+00,\n",
       "       -1.3585e+00,  4.9767e+00,  4.6014e+00,  1.8252e+00, -9.3133e-01,\n",
       "       -4.7533e+00, -3.0548e+00,  3.4432e-01, -3.3965e+00, -2.8556e-01,\n",
       "        5.5976e-01, -2.4043e+00,  2.4901e-02, -1.7460e+00, -2.4686e+00,\n",
       "        5.2176e+00, -7.7651e-02,  4.1721e-01,  9.3170e-01, -3.6655e+00,\n",
       "        9.2885e-01, -5.1232e+00, -1.4109e+00, -2.2130e+00,  1.2593e-01,\n",
       "        1.7002e-01,  3.6620e-01, -3.3297e+00, -2.8706e+00, -3.5471e+00,\n",
       "        2.8726e+00, -1.9366e+00,  2.2992e+00, -1.2327e+00,  2.2837e+00,\n",
       "       -4.9106e+00,  3.7301e+00,  5.5283e+00,  4.2388e+00, -2.1416e+00,\n",
       "        3.8090e+00, -2.8421e+00,  4.9338e+00, -1.4347e+00, -1.7137e+00,\n",
       "        3.0704e+00,  3.1565e+00, -2.9751e+00,  2.4606e+00,  4.1388e+00,\n",
       "        4.0442e-01,  2.2151e+00, -1.1582e+00, -1.2870e+00,  1.3246e+00,\n",
       "       -3.9480e+00, -3.0005e+00,  3.3863e+00, -2.7484e+00, -1.3876e+00,\n",
       "        1.0230e+00, -2.6053e+00, -6.9901e+00, -2.2942e+00, -6.8935e+00,\n",
       "       -4.1983e+00,  9.8276e-01,  1.3395e+00, -4.8869e-02, -1.2287e+00,\n",
       "        1.4470e+00, -7.9510e-01,  4.5749e+00,  6.5947e-01,  2.2056e+00,\n",
       "        2.6567e+00, -4.8067e+00,  4.3957e+00, -1.2664e+00, -1.5321e+00,\n",
       "        4.6449e+00, -3.8600e-01,  4.4860e-01,  9.9003e+00, -1.8173e+00,\n",
       "        1.5570e+00, -2.0185e+00, -2.5939e-01,  7.0708e+00,  2.1668e+00,\n",
       "       -5.5965e+00, -3.6880e+00,  1.0096e+00, -5.1063e+00, -3.2259e-01,\n",
       "        3.9952e-01, -5.6467e+00,  5.2380e+00,  1.7802e-01, -2.3491e-01,\n",
       "       -6.9355e-01, -3.3943e+00,  1.0203e+00,  1.3879e+00,  3.7037e+00,\n",
       "       -1.1950e-01, -4.8030e+00, -3.3641e+00,  3.7098e+00,  3.9947e-01,\n",
       "       -3.6545e+00,  2.5716e+00, -1.3741e+00, -1.6315e+00,  2.0852e+00,\n",
       "        2.8567e-01, -2.0594e+00,  2.6534e+00, -6.3363e+00,  1.8357e+00,\n",
       "       -1.4042e+00, -3.6166e-01,  1.3117e+00, -9.2785e-01,  3.6175e+00,\n",
       "       -2.6949e+00, -2.2252e+00,  5.5984e-02, -3.1778e+00,  4.1249e-01,\n",
       "        6.4716e+00, -1.3846e+00, -2.0004e+00, -4.4885e-01, -3.6592e+00,\n",
       "        2.4708e+00,  6.9389e+00, -4.7911e+00, -1.1108e+00,  4.3447e+00,\n",
       "       -3.4196e+00, -7.2039e-01,  2.5732e+00, -3.7553e-01, -1.4460e+00,\n",
       "       -6.7010e-01,  1.0171e+00,  2.8546e+00, -4.2695e+00,  1.4214e+00,\n",
       "        1.5802e+00,  1.7597e+00, -6.0806e-01, -6.6107e+00,  9.3832e-03,\n",
       "       -4.2763e+00, -5.0507e-01,  5.0049e+00, -8.5312e+00, -1.4967e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
