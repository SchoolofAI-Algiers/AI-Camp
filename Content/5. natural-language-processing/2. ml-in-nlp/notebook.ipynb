{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Language Modeling with N-grams"
      ],
      "metadata": {
        "id": "6q4aPKFFIRUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TA6AvemPIDU3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk import download\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJsWzYcPKUXa",
        "outputId": "8a7a8938-5c80-4353-da00-49921994fc52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Shakespeare texts from NLTK\n",
        "download('gutenberg')\n",
        "shakespeare_texts = ['shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']\n",
        "shakespeare_corpus = [gutenberg.words(text) for text in shakespeare_texts]\n",
        "shakespeare_training_text = ' '.join([' '.join(text) for text in shakespeare_corpus])\n",
        "\n",
        "# Tokenize the training text\n",
        "tokens = word_tokenize(shakespeare_training_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9OJg7OCIfIz",
        "outputId": "bdf750c9-908a-429b-a278-f93ccb8d146c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate n-grams\n",
        "def generate_ngrams(text, n):\n",
        "    ngrams_list = ngrams(text, n)\n",
        "    return list(ngrams_list)\n"
      ],
      "metadata": {
        "id": "Z8tZ9W6UI5u7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate trigrams from the training text\n",
        "trigrams = generate_ngrams(tokens, 3)\n"
      ],
      "metadata": {
        "id": "KyQFJFK4JGqy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a frequency distribution of trigrams\n",
        "freq_dist = FreqDist(trigrams)\n",
        "freq_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOJOtQNdJNtQ",
        "outputId": "9ccad5f8-50d8-45d2-b74d-d74ef084d997"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({(\"'\", 'd', ','): 115, ('.', 'Exeunt', '.'): 71, ('?', 'Ham', '.'): 62, ('my', 'Lord', ','): 60, (',', 'my', 'Lord'): 59, (\"'\", 'th', \"'\"): 55, ('Lord', 'Ham', '.'): 44, (',', 'and', 'the'): 43, ('Ham', '.', 'I'): 42, ('my', 'Lord', 'Ham'): 40, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample testing text\n",
        "testing_text = \"To be or not to be, that is the question.\"\n",
        "\n",
        "# Tokenize the testing text\n",
        "test_tokens = word_tokenize(testing_text)\n",
        "test_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnUwnhF5Jihb",
        "outputId": "6fd57630-112a-418a-b89c-3eba36f9f0e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['To',\n",
              " 'be',\n",
              " 'or',\n",
              " 'not',\n",
              " 'to',\n",
              " 'be',\n",
              " ',',\n",
              " 'that',\n",
              " 'is',\n",
              " 'the',\n",
              " 'question',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word using the trained N-gram model\n",
        "def predict_next_word(context, n):\n",
        "    context_ngram = tuple(context[-n:])\n",
        "    possible_next_words = [ngram[-1] for ngram in freq_dist if ngram[:-1] == context_ngram]\n",
        "    if not possible_next_words:\n",
        "        return None\n",
        "    return random.choice(possible_next_words)\n"
      ],
      "metadata": {
        "id": "d6I5sMv6JoUf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the N-gram model\n",
        "context = ['To', 'your']  # Using the first two words as context\n",
        "ngram_order = 3  # Using trigrams\n",
        "\n",
        "for _ in range(5):  # Generate 5 words\n",
        "    next_word = predict_next_word(context, ngram_order)\n",
        "    if next_word:\n",
        "        print(\" \".join(context), \"->\", next_word)\n",
        "        context.append(next_word)\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2LVYoq2IeLl",
        "outputId": "21735338-e167-490a-a21b-515270472395"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To your -> proceeding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text Classification using Naive Bayes & Logistic Regression"
      ],
      "metadata": {
        "id": "9C3Xs2imNn_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n"
      ],
      "metadata": {
        "id": "S2F2C-WpN-Js"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n"
      ],
      "metadata": {
        "id": "W0dKjMNqN-9v"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the raw text into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "vtMoI0jiOEzd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_counts, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test_counts)\n"
      ],
      "metadata": {
        "id": "-pe0jbCvOHMO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcuTxSZNONIR",
        "outputId": "4aa6005e-c62f-458d-dfe0-a5c8a6bafe8a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Logistic Regression classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_counts, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test_counts)"
      ],
      "metadata": {
        "id": "15gSB95uOPM_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6skve0K-OqAP",
        "outputId": "9b9e980c-b5a4-45a7-d10c-68f370c76880"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TF-IDF for Information Retrieval"
      ],
      "metadata": {
        "id": "hrLq1D-aQHHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "PkUfo6hhQLPh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Reuters corpus from nltk\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Load Reuters news articles as documents\n",
        "documents = [reuters.raw(file_id) for file_id in reuters.fileids()]\n",
        "\n",
        "# Tokenize the documents into sentences\n",
        "tokenized_documents = [sent_tokenize(doc) for doc in documents]\n",
        "\n",
        "# Flatten the list of sentences to create chunks\n",
        "chunks = [sentence for doc in tokenized_documents for sentence in doc]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRyTEkRRQLr_",
        "outputId": "584cbee7-de1e-4019-b82d-1c81636ba304"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(chunks)"
      ],
      "metadata": {
        "id": "W3bCXXv6QUIA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_most_similar(input_text, tfidf_matrix, tfidf_vectorizer, documents):\n",
        "    # Transform the input text using the same vectorizer\n",
        "    input_vector = tfidf_vectorizer.transform([input_text])\n",
        "\n",
        "    # Calculate cosine similarity between the input vector and the document vectors\n",
        "    similarity_scores = cosine_similarity(input_vector, tfidf_matrix)\n",
        "\n",
        "    # Get the index of the most similar chunk\n",
        "    most_similar_index = similarity_scores.argmax()\n",
        "\n",
        "    # Retrieve the most similar chunk of text from the documents\n",
        "    most_similar_text = documents[most_similar_index]\n",
        "\n",
        "    return most_similar_text\n"
      ],
      "metadata": {
        "id": "aSy1-j_GQXC3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input text\n",
        "input_text = \"Economic growth and trade policies impact global markets.\"\n",
        "\n",
        "# Retrieve the most similar chunk of text\n",
        "result = retrieve_most_similar(input_text, tfidf_matrix, tfidf_vectorizer, chunks)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input Text:\")\n",
        "print(input_text)\n",
        "print(\"\\nMost Similar Chunk:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwyU7j9AQaPh",
        "outputId": "94d7c698-deaf-42ae-82e5-6c0da4a7202b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:\n",
            "Economic growth and trade policies impact global markets.\n",
            "\n",
            "Most Similar Chunk:\n",
            "Conable said the World Bank has been pressing developing\n",
            "  countries to open their markets, arguing that a free trading\n",
            "  environment increased the possibility of global economic\n",
            "  growth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyQcQjR1Qd2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CRF for Named Entity Recognition"
      ],
      "metadata": {
        "id": "Et8N_IPgQl0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn-crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6-LIbGYQ8k3",
        "outputId": "e3ad09a2-3aa9-459c-e181-ba1ac71e6bfc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (4.66.2)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "NmpgnASeQpS-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CoNLL 2003 dataset for NER\n",
        "nltk.download('conll2002')\n",
        "\n",
        "# Load the CoNLL 2002 dataset\n",
        "from nltk.corpus import conll2002\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_qfofuiQ65h",
        "outputId": "7dd2e159-2985-41af-ac60-a84a7aa4ff6b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from a sentence\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "wKA50UepRDzZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for word, pos, label in sent]\n"
      ],
      "metadata": {
        "id": "SfbGjhmrRHG9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CoNLL 2002 dataset\n",
        "train_sents = list(conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(conll2002.iob_sents('esp.testb'))\n",
        "\n",
        "# Extract features and labels\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n"
      ],
      "metadata": {
        "id": "5GFklcZ5RK6H"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CRF model\n",
        "crf = sklearn_crfsuite.CRF()\n",
        "crf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "N-emTiLERRlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Flatten the true labels and predicted labels\n",
        "y_test_flat = [label for sent_labels in y_test for label in sent_labels]\n",
        "y_pred_flat = [label for sent_labels in y_pred for label in sent_labels]\n",
        "\n",
        "# Generate and print classification report\n",
        "report = classification_report(y_test_flat, y_pred_flat)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApdLemPvRp_r",
        "outputId": "db060f62-61e6-4af1-a0c0-01eec9957791"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.76      0.77      1084\n",
            "      B-MISC       0.71      0.47      0.57       339\n",
            "       B-ORG       0.79      0.81      0.80      1400\n",
            "       B-PER       0.82      0.84      0.83       735\n",
            "       I-LOC       0.68      0.64      0.66       325\n",
            "      I-MISC       0.62      0.55      0.58       557\n",
            "       I-ORG       0.83      0.79      0.81      1104\n",
            "       I-PER       0.89      0.94      0.92       634\n",
            "           O       0.99      1.00      0.99     45355\n",
            "\n",
            "    accuracy                           0.97     51533\n",
            "   macro avg       0.79      0.75      0.77     51533\n",
            "weighted avg       0.97      0.97      0.97     51533\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxJLT0M8Rwi1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}