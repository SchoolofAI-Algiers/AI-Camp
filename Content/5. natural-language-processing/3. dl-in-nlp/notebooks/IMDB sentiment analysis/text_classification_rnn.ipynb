{"cells":[{"cell_type":"markdown","metadata":{"id":"9TnJztDZGw-n"},"source":["# Text classification with an RNN"]},{"cell_type":"markdown","metadata":{"id":"MjybSpV_GNBi"},"source":[]},{"cell_type":"markdown","metadata":{"id":"lUWearf0Gw-p"},"source":["This text classification tutorial trains several model architectures to see the difference between them and pick the best for this task, and we have:\n","1. Model with simple neural network (no RNN layers)\n","2. Model with the basic RNN layer offered by tensorflow\n","3. Model with GRUs\n","4. Model with LSTMs\n","5. Model with LSTMs + Bidirectional layers\n","\n","on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."]},{"cell_type":"markdown","metadata":{"id":"_2VQo4bajwUU"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z682XYsrjkY9"},"outputs":[],"source":["import numpy as np\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","tfds.disable_progress_bar()"]},{"cell_type":"markdown","metadata":{"id":"pRmMubr0jrE2"},"source":["## Setup input pipeline\n","\n","\n","The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment, represented by the labels 1 and 0.\n","\n","Just for show, we used this dataset from the TFDS library preinstalled in this notebook, otherwise you can work on other labeled text data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56660,"status":"ok","timestamp":1708190891918,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"SHRwRoP2nVHX","outputId":"6bc382b1-a69b-4395-e882-5ea3dab87fbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n","Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"]},{"data":{"text/plain":["(TensorSpec(shape=(), dtype=tf.string, name=None),\n"," TensorSpec(shape=(), dtype=tf.int64, name=None))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset, info = tfds.load('imdb_reviews', with_info=True,\n","                          as_supervised=True)\n","train_dataset, test_dataset = dataset['train'], dataset['test']\n","\n","train_dataset.element_spec"]},{"cell_type":"markdown","metadata":{"id":"nWA4c2ir7g6p"},"source":["Initially this returns a dataset of (text, label pairs):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1708190891919,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"vd4_BGKyurao","outputId":"e69a474f-7a1e-4b64-c77a-ed21273697ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n","label:  0\n"]}],"source":["for example, label in train_dataset.take(1):\n","  print('text: ', example.numpy())\n","  print('label: ', label.numpy())"]},{"cell_type":"markdown","metadata":{"id":"z2qVJzcEluH_"},"source":["Next shuffle the data for training and create batches of these `(text, label)` pairs:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDsCaZCDYZgm"},"outputs":[],"source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","history_list = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VznrltNOnUc5"},"outputs":[],"source":["train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":983,"status":"ok","timestamp":1708190892882,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"jqkvdcFv41wC","outputId":"ee84a0bc-b8da-4a80-fb0c-4e6428c38298"},"outputs":[{"name":"stdout","output_type":"stream","text":["texts:  [b'REALLY??? <br /><br />I am truly amazed to see the glowing reviews here! <br /><br />This is one of the worst movies I have ever seen. It is one big pathetic, grainy, clich\\xc3\\xa9. I would have laughed out loud, and a lot, but was on a date with an ex-military guy. I could not hide my other response, BOREDOM. Yes, I think my date, a flat-line \"good old boy\", liked it. That\\'s not a compliment. I know an actor wants to work.... Fine for the others. But Ralph, come on.<br /><br />It was a painful tease from Ralph. I vote a 2 only because Ralph looked SO STUNNING. But I must plead, Ralph, how could you? And, why?? <br /><br />I\\'m going to go watch The End of The Affair to heal and recover now.... C1'\n"," b'I guess this is meant to be a sort of reworking or updating of \"Beauty and the Beast\", but I can\\'t say I\\'ve ever watched a movie that began with several minutes of graphic horse sex. Wow. Anyway it seems that a young woman and her..aunt? Have traveled to this castle in France where the woman is to be married to the son of the castle owner, who is the man who takes care of making sure the horses get their rocks off. It seems that there are legends in that area of a beast that was rather, uh, frisky, I guess you could say, with the ladies, or at least, one in particular. There are all kinds of references tucked away in that regard but every time the soon-to-be-blushing young bride gets her curious little hands on one the groom\\'s father removes it from her sight. Anyway, the young bride-to-be goes upstairs to sleep while the family is waiting for a Cardinal to show up to the wedding (a family member, I guess) and as she dreams she dreams of a beast in the woods that has its way with her. The effects in this leave a little to be desired, and any attempt at eroticism (not that I know much about that) is kind of rendered laughable, especially when certain featured appendages appear about as realistic as a bed post or a baseball bat. This has a rather strange and abrupt, yet twist ending, with not really any clues or much build up to it, but it was kind of fitting and definitely not what I expected. I don\\'t know, this is kind of a tough one to get through but it has its moments and is definitely weird. 7 out of 10.'\n"," b\"I didn't know Willem Dafoe was so hard up for bucks that he'd disgrace himself with such shocking hamming in this monstrosity. Hell: I'll donate that money that I was going to send to Ethiopia if he's that desperate. I have never seen such a pathetic and disgusting film for a long time...who paid for this? They are either pulling some tax scam or insane. A 5-year old would be ashamed of the plot, and I'd rather get cancer than sit through more than the hour I suffered already. Everybody involved should be locked up for a year in the sodomy wing of a third world prison. Avoid at all costs. I'd give it minus 10 if possible...unbelievable.\"]\n","\n","labels:  [0 1 0]\n"]}],"source":["for example, label in train_dataset.take(2):\n","  print('texts: ', example.numpy()[:3])\n","  print()\n","  print('labels: ', label.numpy()[:3])"]},{"cell_type":"markdown","metadata":{"id":"s5eWCo88voPY"},"source":["## Create the text encoder\n","As you've seen in the previous workshop, the text encoding phase is one of the steps of text preprocessing before we put it in the input layer of our model (or the training data in ML models), here we'll use the text encoder layer of tensorflow which will replace words (or tokens) by numbers based on the preset VOCAB_SIZE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uC25Lu1Yvuqy"},"outputs":[],"source":["VOCAB_SIZE = 1000\n","encoder = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE)\n","encoder.adapt(train_dataset.map(lambda text, label: text))"]},{"cell_type":"markdown","metadata":{"id":"IuQzVBbe3Ldu"},"source":["The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1708190897833,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"tBoyjjWg0Ac9","outputId":"b3d4e754-36f6-42a0-9e30-540f7d2b4edc"},"outputs":[{"data":{"text/plain":["array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n","       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n","      dtype='<U14')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["vocab = np.array(encoder.get_vocabulary())\n","vocab[:20]"]},{"cell_type":"markdown","metadata":{"id":"mjId5pua3jHQ"},"source":["Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1708190897834,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"RGc7C9WiwRWs","outputId":"4669745d-05e2-44bc-d5ac-6d0060ddf828"},"outputs":[{"data":{"text/plain":["array([[ 63,  13,  13, ...,   0,   0,   0],\n","       [ 10, 479,  11, ...,   0,   0,   0],\n","       [ 10, 153, 118, ...,   0,   0,   0]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["encoded_example = encoder(example)[:3].numpy()\n","encoded_example"]},{"cell_type":"markdown","metadata":{"id":"F5cjz0bS39IN"},"source":["With the default settings, the process is not completely reversible. There are three main reasons for that:\n","\n","1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n","2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1708190897834,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"N_tD0QY5wXaK","outputId":"5a085469-380f-4cda-8c84-12f485ec26c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original:  b'REALLY??? <br /><br />I am truly amazed to see the glowing reviews here! <br /><br />This is one of the worst movies I have ever seen. It is one big pathetic, grainy, clich\\xc3\\xa9. I would have laughed out loud, and a lot, but was on a date with an ex-military guy. I could not hide my other response, BOREDOM. Yes, I think my date, a flat-line \"good old boy\", liked it. That\\'s not a compliment. I know an actor wants to work.... Fine for the others. But Ralph, come on.<br /><br />It was a painful tease from Ralph. I vote a 2 only because Ralph looked SO STUNNING. But I must plead, Ralph, how could you? And, why?? <br /><br />I\\'m going to go watch The End of The Affair to heal and recover now.... C1'\n","Round-trip:  really br br i am truly [UNK] to see the [UNK] reviews here br br this is one of the worst movies i have ever seen it is one big [UNK] [UNK] [UNK] i would have [UNK] out [UNK] and a lot but was on a [UNK] with an [UNK] guy i could not [UNK] my other [UNK] [UNK] yes i think my [UNK] a [UNK] good old boy liked it thats not a [UNK] i know an actor wants to work fine for the others but [UNK] come [UNK] br it was a [UNK] [UNK] from [UNK] i [UNK] a 2 only because [UNK] looked so [UNK] but i must [UNK] [UNK] how could you and why br br im going to go watch the end of the [UNK] to [UNK] and [UNK] now [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n","\n","Original:  b'I guess this is meant to be a sort of reworking or updating of \"Beauty and the Beast\", but I can\\'t say I\\'ve ever watched a movie that began with several minutes of graphic horse sex. Wow. Anyway it seems that a young woman and her..aunt? Have traveled to this castle in France where the woman is to be married to the son of the castle owner, who is the man who takes care of making sure the horses get their rocks off. It seems that there are legends in that area of a beast that was rather, uh, frisky, I guess you could say, with the ladies, or at least, one in particular. There are all kinds of references tucked away in that regard but every time the soon-to-be-blushing young bride gets her curious little hands on one the groom\\'s father removes it from her sight. Anyway, the young bride-to-be goes upstairs to sleep while the family is waiting for a Cardinal to show up to the wedding (a family member, I guess) and as she dreams she dreams of a beast in the woods that has its way with her. The effects in this leave a little to be desired, and any attempt at eroticism (not that I know much about that) is kind of rendered laughable, especially when certain featured appendages appear about as realistic as a bed post or a baseball bat. This has a rather strange and abrupt, yet twist ending, with not really any clues or much build up to it, but it was kind of fitting and definitely not what I expected. I don\\'t know, this is kind of a tough one to get through but it has its moments and is definitely weird. 7 out of 10.'\n","Round-trip:  i guess this is meant to be a sort of [UNK] or [UNK] of beauty and the [UNK] but i cant say ive ever watched a movie that [UNK] with several minutes of [UNK] [UNK] sex [UNK] anyway it seems that a young woman and [UNK] have [UNK] to this [UNK] in [UNK] where the woman is to be [UNK] to the son of the [UNK] [UNK] who is the man who takes care of making sure the [UNK] get their [UNK] off it seems that there are [UNK] in that [UNK] of a [UNK] that was rather [UNK] [UNK] i guess you could say with the [UNK] or at least one in particular there are all [UNK] of [UNK] [UNK] away in that [UNK] but every time the [UNK] young [UNK] gets her [UNK] little hands on one the [UNK] father [UNK] it from her [UNK] anyway the young [UNK] goes [UNK] to [UNK] while the family is [UNK] for a [UNK] to show up to the [UNK] a family [UNK] i guess and as she [UNK] she [UNK] of a [UNK] in the [UNK] that has its way with her the effects in this leave a little to be [UNK] and any attempt at [UNK] not that i know much about that is kind of [UNK] [UNK] especially when certain [UNK] [UNK] appear about as realistic as a [UNK] [UNK] or a [UNK] [UNK] this has a rather strange and [UNK] yet twist ending with not really any [UNK] or much [UNK] up to it but it was kind of [UNK] and definitely not what i expected i dont know this is kind of a [UNK] one to get through but it has its moments and is definitely weird [UNK] out of 10                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n","\n","Original:  b\"I didn't know Willem Dafoe was so hard up for bucks that he'd disgrace himself with such shocking hamming in this monstrosity. Hell: I'll donate that money that I was going to send to Ethiopia if he's that desperate. I have never seen such a pathetic and disgusting film for a long time...who paid for this? They are either pulling some tax scam or insane. A 5-year old would be ashamed of the plot, and I'd rather get cancer than sit through more than the hour I suffered already. Everybody involved should be locked up for a year in the sodomy wing of a third world prison. Avoid at all costs. I'd give it minus 10 if possible...unbelievable.\"\n","Round-trip:  i didnt know [UNK] [UNK] was so hard up for [UNK] that [UNK] [UNK] himself with such [UNK] [UNK] in this [UNK] hell ill [UNK] that money that i was going to [UNK] to [UNK] if hes that [UNK] i have never seen such a [UNK] and [UNK] film for a long [UNK] [UNK] for this they are either [UNK] some [UNK] [UNK] or [UNK] a [UNK] old would be [UNK] of the plot and id rather get [UNK] than sit through more than the hour i [UNK] already [UNK] involved should be [UNK] up for a year in the [UNK] [UNK] of a third world [UNK] avoid at all [UNK] id give it [UNK] 10 if [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n","\n"]}],"source":["for n in range(3):\n","  print(\"Original: \", example[n].numpy())\n","  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"bjUqGVBxGw-t"},"source":["## Create the models\n","Each model will have a defined architecture, will be compiled, trained and evaluated, and to conclude the notebook, a graphic plot will be displayed showing the difference of accuracy and evolution of each model\n","### Model 1 (Simple ANN)"]},{"cell_type":"markdown","metadata":{"id":"MeUMC-g0KJw-"},"source":["### Architecture\n","`.Embedding` layer is the next step after word encoding, it'll take each number in the vocabulary and transforms it into a `n` dimension array (here we used 64 dimensions)\n","`.GlobalAveragePooling1D` is used to transform data into 1-dim array (used to make coherence between the layers)\n","`.Dense` + `activation='relu'` frequently used in neural networks for its high precision and calculations\n","`.Dropout` used to avoid overfitting\n","`.Dense(1)` is the output layer because we have two classes to predict them with sigmoid activation function  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXkXQByvAajT"},"outputs":[],"source":["Simple_Ann = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(20, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"markdown","metadata":{"id":"GTpRD6_jLkRo"},"source":["### Compile the model\n","with BCE loss function (binary classification), `.Adam` optimizer for back-propagation with a learning rate of 0.001.\n","acurracy metric is used generally in classification tasks (mesure the number of true predicted classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_fgKgzIAwEv"},"outputs":[],"source":["Simple_Ann.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"GQjK-CUIMeUl"},"source":[]},{"cell_type":"markdown","metadata":{"id":"GfZSNY28Met6"},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75944,"status":"ok","timestamp":1708190974344,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"MMIbkG2rAySP","outputId":"cf51c9fb-d263-43ff-8389-c8b73ab02000"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","391/391 [==============================] - 13s 29ms/step - loss: 0.6905 - accuracy: 0.5000 - val_loss: 0.6863 - val_accuracy: 0.4917\n","Epoch 2/5\n","391/391 [==============================] - 11s 28ms/step - loss: 0.6768 - accuracy: 0.5000 - val_loss: 0.6641 - val_accuracy: 0.4917\n","Epoch 3/5\n","391/391 [==============================] - 11s 28ms/step - loss: 0.6436 - accuracy: 0.5120 - val_loss: 0.6185 - val_accuracy: 0.5130\n","Epoch 4/5\n","391/391 [==============================] - 11s 29ms/step - loss: 0.5918 - accuracy: 0.6001 - val_loss: 0.5601 - val_accuracy: 0.6760\n","Epoch 5/5\n","391/391 [==============================] - 12s 30ms/step - loss: 0.5368 - accuracy: 0.6942 - val_loss: 0.5060 - val_accuracy: 0.7224\n"]}],"source":["trained_Simple_Ann = Simple_Ann.fit(train_dataset, epochs=5,\n","                    validation_data=test_dataset,\n","                    validation_steps=30)\n"]},{"cell_type":"markdown","metadata":{"id":"2LnIrraBN8AK"},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7684,"status":"ok","timestamp":1708190982003,"user":{"displayName":"Youcef Benali","userId":"17020805508400996963"},"user_tz":-60},"id":"qCJmJr7lN_0e","outputId":"ce1b3221-23e4-4a20-9855-dfe7ee1e83e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["391/391 [==============================] - 8s 19ms/step - loss: 0.5040 - accuracy: 0.7287\n","Test Loss: 0.5039566159248352\n","Test Accuracy: 0.7287200093269348\n"]}],"source":["test_loss_Simple_Ann, test_acc_Simple_Ann = Simple_Ann.evaluate(test_dataset)\n","\n","print('Test Loss:', test_loss_Simple_Ann)\n","print('Test Accuracy:', test_acc_Simple_Ann)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoatkeYKOwl2"},"outputs":[],"source":["history_list.append(trained_Simple_Ann)"]},{"cell_type":"markdown","metadata":{"id":"GieLh7tKPNvi"},"source":["### Model 2: Simple Recurrent neural network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwfoBkmRYcP3"},"outputs":[],"source":["RNN_model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),\n","    tf.keras.layers.SimpleRNN(32),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaegPaWia3Xn"},"outputs":[],"source":["RNN_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xWuDZueoa3NB","outputId":"f2d0c87b-de1f-4be9-df46-9a76a9753409"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","391/391 [==============================] - 164s 416ms/step - loss: 0.6927 - accuracy: 0.5001 - val_loss: 0.6859 - val_accuracy: 0.4917\n","Epoch 2/5\n","391/391 [==============================] - 158s 403ms/step - loss: 0.6438 - accuracy: 0.5888 - val_loss: 0.6122 - val_accuracy: 0.6385\n","Epoch 3/5\n","391/391 [==============================] - 147s 375ms/step - loss: 0.5400 - accuracy: 0.7476 - val_loss: 0.5588 - val_accuracy: 0.7344\n","Epoch 4/5\n","391/391 [==============================] - 156s 398ms/step - loss: 0.4458 - accuracy: 0.8088 - val_loss: 0.4326 - val_accuracy: 0.8083\n","Epoch 5/5\n","391/391 [==============================] - 157s 402ms/step - loss: 0.4791 - accuracy: 0.7999 - val_loss: 0.4097 - val_accuracy: 0.8313\n"]}],"source":["trained_RNN_model = RNN_model.fit(train_dataset, epochs=5,\n","                    validation_data=test_dataset,\n","                    validation_steps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HIgDH9cJ7zZB","outputId":"a64e2c31-7497-4c26-c345-eb8be4f6941a"},"outputs":[{"name":"stdout","output_type":"stream","text":["391/391 [==============================] - 30s 78ms/step - loss: 0.4062 - accuracy: 0.8324\n","Test Loss: 0.40619397163391113\n","Test Accuracy: 0.8324000239372253\n"]}],"source":["test_loss_RNN_model, test_acc_RNN_model = RNN_model.evaluate(test_dataset)\n","\n","print('Test Loss:', test_loss_RNN_model)\n","print('Test Accuracy:', test_acc_RNN_model)\n","history_list.append(trained_RNN_model)"]},{"cell_type":"markdown","metadata":{"id":"jBL3ipLV8rZe"},"source":["### Model 3: GRU model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vEUcd_Qb8yyZ"},"outputs":[],"source":["GRU_model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),\n","    tf.keras.layers.GRU(32),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"byR3J2oi8-65"},"outputs":[],"source":["GRU_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CVKNn_eG9Di4","outputId":"69dec41e-91e3-4315-a12d-668674e793e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","391/391 [==============================] - 239s 599ms/step - loss: 0.6906 - accuracy: 0.5000 - val_loss: 0.6854 - val_accuracy: 0.4917\n","Epoch 2/5\n","391/391 [==============================] - 235s 599ms/step - loss: 0.5480 - accuracy: 0.6692 - val_loss: 0.4543 - val_accuracy: 0.7740\n","Epoch 3/5\n","391/391 [==============================] - 233s 595ms/step - loss: 0.3948 - accuracy: 0.8257 - val_loss: 0.4027 - val_accuracy: 0.8062\n","Epoch 4/5\n","391/391 [==============================] - 235s 601ms/step - loss: 0.3582 - accuracy: 0.8477 - val_loss: 0.3614 - val_accuracy: 0.8411\n","Epoch 5/5\n","391/391 [==============================] - 234s 599ms/step - loss: 0.3416 - accuracy: 0.8572 - val_loss: 0.3509 - val_accuracy: 0.8474\n"]}],"source":["trained_GRU_model = GRU_model.fit(train_dataset, epochs=5,\n","                    validation_data=test_dataset,\n","                    validation_steps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dQZMvudN-H_B","outputId":"538459b7-9928-4ef4-ac8c-24f2aaa1b676"},"outputs":[{"name":"stdout","output_type":"stream","text":["391/391 [==============================] - 48s 123ms/step - loss: 0.3452 - accuracy: 0.8444\n","Test Loss: 0.3451537787914276\n","Test Accuracy: 0.8444399833679199\n"]}],"source":["test_loss_GRU_model, test_acc_GRU_model = GRU_model.evaluate(test_dataset)\n","\n","print('Test Loss:', test_loss_GRU_model)\n","print('Test Accuracy:', test_acc_GRU_model)\n","history_list.append(trained_GRU_model)"]},{"cell_type":"markdown","metadata":{"id":"gNY22j-G-fLy"},"source":["### Model 4: LSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dqzn494E-kBM"},"outputs":[],"source":["LSTM_model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"91GH5loy-pe9"},"outputs":[],"source":["LSTM_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-3),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SMi7avwW-rv3","outputId":"f6c8d137-c703-42a0-919f-3da870771d73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","391/391 [==============================] - 263s 661ms/step - loss: 0.5152 - accuracy: 0.7293 - val_loss: 0.4149 - val_accuracy: 0.8104\n","Epoch 2/5\n","391/391 [==============================] - 252s 644ms/step - loss: 0.4616 - accuracy: 0.8000 - val_loss: 0.3858 - val_accuracy: 0.8344\n","Epoch 3/5\n","391/391 [==============================] - 252s 645ms/step - loss: 0.4286 - accuracy: 0.8248 - val_loss: 0.3813 - val_accuracy: 0.8359\n","Epoch 4/5\n","391/391 [==============================] - 253s 648ms/step - loss: 0.4189 - accuracy: 0.7871 - val_loss: 0.6212 - val_accuracy: 0.5349\n","Epoch 5/5\n","391/391 [==============================] - 251s 641ms/step - loss: 0.4790 - accuracy: 0.7580 - val_loss: 0.4404 - val_accuracy: 0.8203\n"]}],"source":["trained_LSTM_model = LSTM_model.fit(train_dataset, epochs=5,\n","                    validation_data=test_dataset,\n","                    validation_steps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qcO1WY_k-xiF","outputId":"417197f9-db43-49f6-f63c-7d74b2ca5c06"},"outputs":[{"name":"stdout","output_type":"stream","text":["391/391 [==============================] - 56s 143ms/step - loss: 0.4479 - accuracy: 0.8174\n","Test Loss: 0.44787338376045227\n","Test Accuracy: 0.817359983921051\n"]}],"source":["test_loss_LSTM_model, test_acc_LSTM_model = LSTM_model.evaluate(test_dataset)\n","\n","print('Test Loss:', test_loss_LSTM_model)\n","print('Test Accuracy:', test_acc_LSTM_model)\n","history_list.append(trained_LSTM_model)"]},{"cell_type":"markdown","metadata":{"id":"lDAak-3b-5b2"},"source":["### Model 5: LSTM + Bidirectional"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kY8qx3_h-8mP"},"outputs":[],"source":["BiLSTM_model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-m56gjXO-8--"},"outputs":[],"source":["BiLSTM_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-3),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZWvPBIK7-9QG"},"outputs":[],"source":["trained_BiLSTM_model = BiLSTM_model.fit(train_dataset, epochs=5,\n","                    validation_data=test_dataset,\n","                    validation_steps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLC6ev0Q--SX"},"outputs":[],"source":["test_loss_BiLSTM_model, test_acc_BiLSTM_model = LSTM_model.evaluate(test_dataset)\n","\n","print('Test Loss:', test_loss_BiLSTM_model)\n","print('Test Accuracy:', test_acc_BiLSTM_model)\n","history_list.append(trained_BiLSTM_model)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb","timestamp":1707415574287}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}